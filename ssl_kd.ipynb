{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "device = torch.device(\"cuda:2\")\n",
    "target_z = torch.load(\"/home/sjoshi/mtt-distillation/target_rep/CIFAR100/train_rep_r50_128_dim.pt\", map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjoshi/anaconda3/envs/distillation/lib/python3.9/site-packages/torchvision/transforms/transforms.py:890: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision \n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "def ColourDistortion(s=1.0):\n",
    "    # s is the strength of color distortion.\n",
    "    color_jitter = transforms.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.2*s)\n",
    "    rnd_color_jitter = transforms.RandomApply([color_jitter], p=0.8)\n",
    "    rnd_gray = transforms.RandomGrayscale(p=0.2)\n",
    "    color_distort = transforms.Compose([rnd_color_jitter, rnd_gray])\n",
    "    return color_distort\n",
    "\n",
    "\n",
    "mean = [0.4914, 0.4822, 0.4465]\n",
    "std = [0.2023, 0.1994, 0.2010]\n",
    "transform_train = transforms.Compose([\n",
    "        transforms.RandomResizedCrop((32,32), interpolation=Image.BICUBIC),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        ColourDistortion(s=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std),\n",
    "    ])\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
    "dataset = torchvision.datasets.CIFAR100(\"/data\", train=True, transform=transform_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from torch.utils.data import Dataset \n",
    "class DatasetWithIndices(Dataset):\n",
    "    def __init__(self, dataset) -> None:\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def __getitem__(self, index: Any) -> Any:\n",
    "        return index, self.dataset[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networks import ConvNet, ResNet18\n",
    "from utils import get_default_convnet_setting\n",
    "trainloader = torch.utils.data.DataLoader(DatasetWithIndices(dataset), batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "device = torch.device(\"cuda:2\")\n",
    "net_width, net_depth, net_act, net_norm, net_pooling = get_default_convnet_setting()\n",
    "model = ConvNet(channel=3, num_classes=128, net_depth=net_depth, net_act=net_act, net_width=net_width, net_norm=net_norm, net_pooling=net_pooling)\n",
    "\n",
    "model = ResNet18(channel=3, num_classes=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pre-training: 100%|██████████| 10/10 [02:02<00:00, 12.24s/it, loss: 0.21730676044464112]\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, optim\n",
    "from tqdm import tqdm \n",
    "import torch.nn.functional as F\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n",
    "pbar = tqdm(range(10), desc=\"pre-training\")\n",
    "for epoch in pbar:\n",
    "    loss_avg, num_exp = 0, 0\n",
    "    for idx, datum in trainloader:\n",
    "        img = datum[0].float().to(device)\n",
    "        n_b = img.shape[0]\n",
    "        student_z = model(img)\n",
    "        \n",
    "        student_dist = F.cosine_similarity(student_z.unsqueeze(1), student_z.unsqueeze(0), dim=2)\n",
    "        teacher_z = target_z[idx]\n",
    "        teacher_dist = F.cosine_similarity(teacher_z.unsqueeze(1), teacher_z.unsqueeze(0), dim=2)\n",
    "        loss = torch.log(torch.sum(torch.exp(student_dist - teacher_dist)))\n",
    "\n",
    "        loss_avg += loss.item()\n",
    "        num_exp += n_b\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    loss_avg /= num_exp\n",
    "    pbar.set_postfix_str(f\"loss: {loss_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding: 100%|██████████| 4/4 [00:00<00:00,  6.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 2048])\n",
      "torch.Size([1000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset \n",
    "import random\n",
    "\n",
    "Z = []\n",
    "Y = []\n",
    "random.seed(0)\n",
    "random_subset = list(random.sample(range(50000), 1000))\n",
    "clf_cifar100 = Subset(torchvision.datasets.CIFAR100(\"/data\", transform=transform), indices=random_subset)\n",
    "clf_dataloader = torch.utils.data.DataLoader(clf_cifar100, batch_size=256, shuffle=False, num_workers=4, pin_memory=True)\n",
    "with torch.no_grad():\n",
    "    for X, y in tqdm(clf_dataloader, desc=\"encoding\"):\n",
    "        Z.append(model.features(X.to(device)).view(-1, 2048))\n",
    "        #Z.append(model(X.to(device)))\n",
    "        Y.append(y.to(device))\n",
    "Z = torch.cat(Z, dim=0)\n",
    "Y = torch.cat(Y, dim=0)\n",
    "print(Z.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_clf(X, y, representation_dim, num_classes, device, reg_weight=1e-3, iter=500):\n",
    "    print('\\nL2 Regularization weight: %g' % reg_weight)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    n_lbfgs_steps = iter\n",
    "\n",
    "    # Should be reset after each epoch for a completely independent evaluation\n",
    "    clf = nn.Linear(representation_dim, num_classes).to(device)\n",
    "    clf_optimizer = optim.LBFGS(clf.parameters())\n",
    "    clf.train()\n",
    "\n",
    "    t = tqdm(range(n_lbfgs_steps), desc='Loss: **** | Train Acc: ****% ', bar_format='{desc}{bar}{r_bar}')\n",
    "    for _ in t:\n",
    "        def closure():\n",
    "            clf_optimizer.zero_grad()\n",
    "            raw_scores = clf(X)\n",
    "            loss = criterion(raw_scores, y)\n",
    "            loss += reg_weight * clf.weight.pow(2).sum()\n",
    "            loss.backward()\n",
    "\n",
    "            _, predicted = raw_scores.max(1)\n",
    "            correct = predicted.eq(y).sum().item()\n",
    "\n",
    "            t.set_description('Loss: %.3f | Train Acc: %.3f%% ' % (loss, 100. * correct / y.shape[0]))\n",
    "\n",
    "            return loss\n",
    "\n",
    "        clf_optimizer.step(closure)\n",
    "\n",
    "    return clf\n",
    "\n",
    "\n",
    "def test_clf(testloader, device, net, clf, features=True):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    net.eval()\n",
    "    clf.eval()\n",
    "    test_clf_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    acc_per_point = []\n",
    "    with torch.no_grad():\n",
    "        t = tqdm(enumerate(testloader), total=len(testloader), desc='Loss: **** | Test Acc: ****% ',\n",
    "                 bar_format='{desc}{bar}{r_bar}')\n",
    "        for batch_idx, (inputs, targets) in t:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            representation = None\n",
    "            if features:\n",
    "                representation = net.features(inputs).view(-1, 2048)\n",
    "            else:\n",
    "                representation = net(inputs)\n",
    "            # test_repr_loss = criterion(representation, targets)\n",
    "            raw_scores = clf(representation)\n",
    "            clf_loss = criterion(raw_scores, targets)\n",
    "            test_clf_loss += clf_loss.item()\n",
    "            _, predicted = raw_scores.max(1)\n",
    "            total += targets.size(0)\n",
    "            acc_per_point.append(predicted.eq(targets))\n",
    "            correct += acc_per_point[-1].sum().item()\n",
    "            t.set_description('Loss: %.3f | Test Acc: %.3f%% ' % (test_clf_loss / (batch_idx + 1), 100. * correct / total))\n",
    "            \n",
    "    acc = 100. * correct / total\n",
    "    return acc, torch.cat(acc_per_point, dim=0).cpu().numpy()\n",
    "\n",
    "def top5accuracy(output, target, topk=(5,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        print(correct)\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].contiguous().view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size).item())\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar100_test = torchvision.datasets.CIFAR100(\"/data\", train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(cifar100_test, batch_size=256, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "L2 Regularization weight: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.269 | Train Acc: 100.000% : ██████████| 100/100 [00:02<00:00, 35.34it/s]\n"
     ]
    }
   ],
   "source": [
    "clf = train_clf(Z, Y, Z.shape[1], 100, device, iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 4.679 | Test Acc: 11.230% : ██████████| 40/40 [00:00<00:00, 61.26it/s]\n"
     ]
    }
   ],
   "source": [
    "acc, acc_per_point = test_clf(testloader, device, model, clf, features=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distillation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
